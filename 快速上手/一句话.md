
# 数学

* 方差：衡量一组数据的波动程度，没有波动就是0。
* 基数：枚举值的数量，例如枚举值越多，基数越高。
* 归一化：把数值缩放到0-1之间。
* 线性回归：找出多个值之间的变化关系。
* 逻辑回归：多个值之间变化关系，得出0/1。
* 决策树：Decision Trees：通过多层判断，得出关系，可用于回归和分类
* 神经网络：多层进行非线性变化，每层需要有下一层的激活函数
* 全连接神经网络：Full Connected：每层都可以到下一层的所有值，需要很大的算力。
* 卷积神经网络：CNN：经过多次卷积和池化迭代。
  * 卷积层：Convolution：作为特征提取器，例如在图片中一个8x8的聚焦框，滑动到不同坐标再给下一层处理。
  * 池化层：Pooling：做采样，降低特征图的大小。
* 循环神经网络：RNN：输入跟上一时间的输入也有关。
* 长短期记忆：LSTM：Long Short-Term Memory：改善长时间错误的影响。

# 机器学习

* 过拟合：Overfitting：训练集表现很好，测试集不行，说明模型复杂度过高。较高方差。
* 欠拟合：训练集和测试集都不行。说明模型较高偏差。
* 学习率：Learning Rate：训练神经网络重要超参数之一，每次迭代中梯度向损失函数最优解移动的步长，通常η表示。决定学习速度快慢。在网络训练过程中，模型通过样本数据给出预测值，计算代价函数并通过反向传播来调整参数。
* 辍学率：Dropout Rate：通过在每次训练时，忽略一半的特征检测器，使模型更泛化，减少过拟合。
* 平坦层：Flatten layer：把输入压平，从多维变单维，常用在从卷积层到全链接层过渡。
* 卷积层：Convolutional layer：卷积神经网络中每层卷积层由若干卷积单元组成，每个卷积单元的参数都通过反向传播算法最佳化得到。卷积运算目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级特征如边缘、线条和角等层级，多层网路能从低级特征中迭代提取更复杂特征。
* 全链接层：Fully connected layers：FC：每个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于全相连，一般参数也最多。在卷积神经网络中起到“分类器”作用。
* 局部最小值：Local minimum：局部最优解。使用随机梯度下降，模拟退火等方法跳出局部最小值。
* 随机梯度下降：随机选择开始计算的点。
* 模拟退火：每步计算中以一定几率接受比当前更差的结果，保证算法稳定。
* 泛化能力：Generalization ability：算法对新样本的适应能力。
* 常用CNN模型：
  * LeNet(1998) 手写体识别
  * AlexNet(2012)
  * VGG-16(2014)
  * ResNet(2015)
* CV 计算机视觉 常见问题
  * 分类：图片属于什么分类
  * 检测：对象在图中什么区域
  * 语义分割/图像分割：对象在图中的边界，绿幕换人

## 特征工程

* 特征缩放：特征归一化。
* PCA：有效成分，特征降维，提炼重要信息。
* GMM：高斯混合模型：只能用于连续特征。
* 多项式分布朴素贝叶斯模型：只能用于category特征。
* xgboost模型统一把特征按连续特征处理。建议还是用于连续特征，离散特征可以转one-hot，但是基数不要大于100。
* 离散特征建议用CatBoost 或 Light GBM。
* target encoding可以对离散特征转连续特征。

# 网络

* BFD：Bidirectional Forwarding Detection：双向转发侦测，用于侦测链路错误的网络协议，可用于链路高可用实时监测。
