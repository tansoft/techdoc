
# 数学

* 偏差：bias：预测与真实的偏离程度，即算法本身的拟合能力。
* 方差：variance：衡量一组数据的波动程度，没有波动就是0。
* 基数：枚举值的数量，例如枚举值越多，基数越高。
* 归一化：把数值缩放到0-1之间。
* 线性回归：找出多个值之间变化关系，通过已知值预测结果，例如预测房价。参数计算方法是最小二乘法。
* 最小二乘法：最小平方法：least squares method：通过已知的一组数据，找出最接近这组数据的二次方程，y=ax+b，即找到这些数据的规律。
* 逻辑回归：Logistic Regression：LR：找出多个值之间变化关系，并映射到0-1区间，从而进行分类，得出0/1。参数计算方法是梯度下降。
* 梯度下降法：Gradient descent：梯度就是当前点的倾斜方向，梯度下降就是逐渐向低的地方找的方法，梯度下降只能找到局部最优解（如进到一个山谷里）。凸函数可以找到全局最优解。
* 梯度上升法：梯度下降相反。
* 批量梯度下降法：Batch Gradient Descent：BGD：所有数据计算梯度，计算量大，训练慢，收敛快，最优解。
* 随机梯度下降法：Stochastic Gradient Descent：SGD：随机一个数据计算梯度，训练快，收敛可能慢，可能不是最优解。
* 小批量梯度下降法：Mini-batch Gradient Descent：MBGD：少量样本，一般10个计算，综合上述两种取舍。
* 凸函数：Convex function：山谷一样的。
* 凹函数：Concave function：山头一样的。
* 梯度下降和最小二乘，一个是直接计算求解，一个是一步一步逼近解，样本量大时最小二乘的逆矩阵会很大，计算不过来，另外无法通过计算解析解的情况下也不能用。
* 决策树：Decision Trees：通过多层判断（if-else），得出关系和最终判断（如白不白，富不富，美不美，最终决定是否相亲），可用于回归和分类
* 随机森林：Random Forest：RF：多个决策树同时决策，选取最多的结果作为最终结果。输入参数可以很多，因此是降维好工具。
* 梯度提升决策树：GBDT：训练一组决策树，每次迭代都用上一模型残差拟合下一模型。最终得到预测结果是所有树预测结果加权总和。
* 随机森林和 GBDT 都由多个决策树组成的模型，可用于回归和分类。两者的区别在于重建和组合的方式。随机森林 bagging 可大幅减少差异和过拟合，而 GBDT boosting 则可减少偏差和欠拟合。
* XGBoost：极端梯度提升：是 GBDT 升级，树并行构建，而非顺序构建。
* 随机砍伐森林：RCF：特殊类型的随机森林，用于检测数据集中异常数据点。异常可表现为时间序列数据中意外峰值、周期性中断或无法分类数据点。
* 曲线下面积：Area Under Curve：AUC：ROC曲线下与坐标轴围成的面积，多用于模型效果评价，取值0-1，0.5表示随机猜测水平，数值越高越准确。
* HardMax：数据集中的最大值。
* Softmax：把数据集中的数据，按大小变为0-1区间的概率，数值越大的数概率值越大，总和为1。多用于多分类预测。
* 贝叶斯定理：事件概率和事件之间概率的关系。P(A|B)=P(B|A)*P(A)/P(B)。其中 P(A|B) 是条件概率，表示当事件B发生的情况下事件A发生的概率。贝叶斯定理中有先验概率和后验概率之分：
  * 先验概率：Prior Probability：根据以往经验和统计分析得到的概率。在“结果”发生前的概率，比如公式中的P(A)就是先验概率。先验概率一般作为“由因求果”问题中的“因”出现。
  * 后验概率：Posterior Probability：根据观察到的样本修正之后的概率值。在结果发生之后，我们根据“结果”来计算和分析最有可能导致该结果的原因，即“执果寻因”中的“因”。公式中的P(A|B)就是后验概率。
  * 患者病菌感染概率P(V)=5%，患者感冒概率P(C)=30%，患者因为病菌感染而感冒的概率P(C|V)=40%，那么得出感冒患者被病菌感染的后验概率P(V|C)=P(C|V)*P(V)/P(C)≈66.67%。
* 贝叶斯网络：Bayesian Network：描述随机变量（事件）之间关系的模型，基于贝叶斯定理对事物之间因果关系以及依赖关系进行量化，并使得因果或依赖关系的强弱可以被推理和计算。用有向无环图表示，节点代表随机变量，箭头表示节点间联系。如表示疾病和症状之间的概率关系。
* 朴素贝叶斯分类器：单纯贝氏分类器：Naive Bayes classifier：假设特征之间强（朴素）独立下运用贝叶斯定理的简单概率分类器。监督学习。例：通过身高、体重、脚尺寸判断男性女性。
* 多项式分布朴素贝叶斯模型：只能用于category特征。
* 泊松分布：Poisson Distribution：适合于描述单位时间内随机事件发生的次数。如：每小时走入商店人数，网络上每分钟丢包数。
* 均匀分布：Uniform Distribution：一种简单的概率分布，分为离散型均匀分布和连续型均匀分布。
* 正态分布/高斯分布：Normal Distribution/Gaussian Distribution：概率密度函数曲线呈钟形。
* 二项分布：Binomial Distribution：n个独立的成功/失败试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。单次试验又称为伯努利试验。实际上，当n=1时，二项分布就是伯努利分布。
* 多重插补：Multiple Imputation：基于重复模拟的处理缺失值方法。每个数据集中的缺失数据用蒙特卡洛方法来填补。
* 均值替代法：Mean Substitution：以变量中未缺失观察值的均数估计该变量中存在的缺失值。当缺乏其他信息时，该方法是对缺失值估计的常用方法。
* Term Frequency-InversDocument Frequency：TF-IDF：常用于信息处理和数据挖掘的加权。字词在文本中出现次数（TF）和在整个语料中出现的文档频率（IDF，逆文档频率）来计算字词在整体的重要程度。优点能过滤常见无关紧要词，同时保留重要字词。
* 混淆矩阵/误差矩阵：Confusion Matrix：精度评价的标准格式，用n行n列的矩阵形式来表示，如真/假阳/阴性。
* K最邻近：KNN：K-Nearest Neighbor：KNN演示Demo：http://vision.stanford.edu/teaching/cs231n-demos/knn/
* K均值聚类算法：K-means clustering algorithm：通过多次迭代求解实现聚类，https://www.cnblogs.com/pinard/p/6164214.html
* 线性学习器：Linear learner：求解分类或回归问题方法。
* LightGBM：Light Gradient Boosting Machine：基于决策树分布式梯度提升框架。基于梯度的单边采样 (GOSS) 和互斥特征捆绑 (EFB)。对过拟合很敏感，小数据集容易过拟合。LightGBM比XGBoost快10倍，内存占用大约为XGBoost的1/6。
* 因子分解机：Factorization Machine：FM：基于矩阵分解的机器学习算法，把握组合的高阶特征，用于广告CTR点击率预估。主要解决特征过多，稀疏特征为0两类问题。在分解矩阵的时候，可以减少数据复杂度，并可进行预测。
* 超参数：Hyperparameter：训练模型时开始指定的不能变化的参数。
* 超参数调优方法：
  * 人工
  * 网格搜索：Grid Search：GS：暴力搜索，在给定超参搜索空间内，尝试所有超参组合，最后搜索出最优的超参组合
  * 随机搜索：Random Search：RS：暴力搜索，在搜索空间中采样出超参组合，然后选出采样组合中最优的超参组合
  * 贝叶斯优化：Bayesian Optimization：BO：使用了 高斯过程：gasussian processes：GP：去构建代理模型（简单很多的模型），降低搜索过程的复杂性
  * SMBO：简洁版的贝叶斯优化
  * Hyperband：HB：本质上是随机搜索的变种，它使用早停策略和Sccessive Halving算法去分配资源，能够快速淘汰表现不佳的组合，因此在相同资源预算下，比贝叶斯方法收敛更快
  * BOHB：混合了贝叶斯优化和Hyperband

# 机器学习

* 神经网络：多层进行非线性变化，每层需要有下一层的激活函数
* 全连接神经网络：Full Connected：FC：每个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于全相连，一般参数也最多，需要很大算力。在卷积神经网络中起到“分类器”作用。
* 卷积神经网络：Convolutional Neural Network：CNN：经过多次卷积和池化迭代。最擅长图片处理：图片大量降维，有效保留图片特征。
  * 每一步进化演示：https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
  * 卷积层：Convolutional layer：每层卷积层由若干卷积单元组成，作为特征提取器，例如在图片中一个8x8的聚焦框，滑动到不同坐标再给下一层处理。卷积运算目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级特征如边缘、线条和角等层级，多层网路能从低级特征中迭代提取更复杂特征。每个卷积单元的参数都通过反向传播算法最佳化得到。
  * 池化层：Pooling：做采样，降低特征图的大小。
* 平坦层：Flatten layer：把输入压平，从多维变单维，常用在从卷积层到全链接层过渡。
* 循环/递归神经网络：Recurrent Neural Network：RNN：输入跟上一时间的输入也有关。
* 长短期记忆：LSTM：Long Short-Term Memory：改善长时间错误的影响。
* 迁移学习：Transfer learning：运用已有知识来学习新知识，找到已有知识和新知识之间相似性。降低从头开始学习的成本。
* 过拟合：Overfitting：训练集表现很好，测试集不行，说明模型复杂度过高。较高方差。
* 欠拟合：训练集和测试集都不行。说明模型较高偏差。
* 学习率：Learning Rate：η：神经网络重要超参，每次迭代梯度向损失函数最优解移动的步长，决定学习速度快慢。在网络训练过程，模型通过样本数据给出预测值，计算代价函数并通过反向传播来调整参数。一般使用0.1，0.01等，从小往大调整，同时观察损失率。大学习率一般还能加强泛化能力。学习率调整策略包括step（常用），multistep（常用），exp，inv，fixed，poly，sigmoid。https://zhuanlan.zhihu.com/p/64864995
* 批大小：batchsize：进一步调整模型效果时比较关键。大batchsize能减少训练时间，提高稳定性。因为batch次数减少，梯度计算更稳定，模型训练曲线更平滑。但是过大batchsize会影响模型泛化能力。
  * 学习率和批大小关系：
  * 分布式学习中，batchsize越大，学习率越大，才能有较好的收敛速度和精度。本质是为了梯度方差保持不变，解决陷入局部最优解。
  * 训练受阻情况，具有多个局部最小值，应减少批处理大小，有助于摆脱局部最小值限制；降低学习率将防止超出全局损失函数最小值。
* 辍学率：Dropout Rate：通过在每次训练时，忽略一半的特征检测器，使模型更泛化，减少过拟合。
* 局部最小值：Local minimum：局部最优解。使用随机梯度下降，模拟退火等方法跳出局部最小值。
* 损失函数：Lost Function：通常用来评估预测值和真实值的差距，通常越小越好。
* 模拟退火：每步计算中以一定几率接受比当前更差的结果，保证算法稳定。
* 泛化能力：Generalization ability：算法对新样本的适应能力。
* 有向无环图：Directed Acyclic Graph：DAG：
* 常用CNN模型：
  * LeNet(1998) 手写体识别
  * AlexNet(2012)
  * VGG-16(2014)
  * ResNet(2015)
* CV 计算机视觉 常见问题
  * 分类：图片属于什么分类
  * 检测：对象在图中什么区域
  * 语义分割/图像分割：对象在图中的边界，绿幕换人
* Horovod：分布式深度学习框架，通过并发加快对Tensorflow、Keras、Pytorch和MXNet等其他深度学习框架的训练，支持数据并行化和模型并行化。
* scikit-learn：Python机器学习工具库。
* 分类：
  * 真阳性：True Positive：TP：正样本成功预测为正。
  * 真阴性：True Negative：TN：负样本成功预测为负。
  * 假阳性：False Positive：FP：负样本错误地预测为正。
  * 假阴性：False Negative：FN：正样本错误的预测为负。
  * 准确率：Accuracy：(TP+TN)/ALL。正确预测阴阳比总数。强调对了多少。在数据不平衡时缺点很大，如直接预测占绝大数量的分类得到很高的准确率。
  * 精准率：Precision：TP/(TP+FP)。预测为阳数据中，真正为阳的比例。强调错了多少。假阳性影响很大时（垃圾邮件判断）着重考虑。
  * 召回率：Recall：TP/(TP+FN)。预测正确的阳占总阳比例。强调漏掉了多少。假阴性影响很大时（如癌症普筛，可错判不可漏）着重考虑。
  * F1得分：F1 Score：2\*Precision\*Recall/(Precision+Recall)。综合了精准和召回，得分越高越好。
  * 多分类性能判断：
    * 宏观平均：Macro-average：多类别加起来求平均，所有类别权重相同。会受稀有类别影响。
    * 加权平均：Weighted-average：每类别乘权重后再相加。考虑了类别不平衡情况，更容易受到常见类（majority class）影响。
    * 微观平均：Micro-average：每个类别的TP, FP, FN先相加之后，在根据二分类的公式进行计算。更多考虑全局的情况。Micro-precision=Micro-recall=Accuracy
* 均方根误差：Root Mean Square Error/Deviation：RMSE/RMSD：平方误差平均值的平方根，衡量模型预测值与观测值之间差异，对大误差很敏感。越低越好，0表示完全重合。各误差对均方根误差影响与平方误差大小成正比，因此较大误差对均方根误差有较大影响，均方根误差对离群值很敏感。
* 剩余图：Residual plots：多用于分析回归模型高估还是低估了目标。观察或预测到的误差error(残差residuals)与随机误差(stochastic error)是否一致，好像骰子一样，不能准确预测（随机误差），但是能算出概率，但如果骰子做了手脚，残差就和随机误差不一致。 https://iphysresearch.github.io/blog/post/ml_notes/residual-plots/

## 特征工程

* 特征缩放：特征归一化。
* 主成分分析/有效成分分析：Principal Component Analysis：PCA：特征降维方法，把存在一定相关关系的多个变量，变成尽可能少的新变量，使这些新变量两两不相关，减少变量数目。
* 高斯混合模型：GMM：只能用于连续特征。
* xgboost模型统一把特征按连续特征处理。建议还是用于连续特征，离散特征可以转one-hot，但是基数不要大于100。
* 离散特征建议用CatBoost 或 Light GBM。
* target encoding可以对离散特征转连续特征。

# 网络

* BFD：Bidirectional Forwarding Detection：双向转发侦测，用于侦测链路错误的网络协议，可用于链路高可用实时监测。
